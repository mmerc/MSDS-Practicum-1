---
title: "Predicting Home Value Based on Structural Features"
author: "Maria Mercier"
date: "June 23. 2018"
output:
  pdf_document: default
  html_document: default
---

# Introduction

Purchasing a home is a major milestone in life.  Individuals spend an enormous amount of time researching the various characteristics of a house to find the best value. One helpful resource would be to predict the value of homes based on the structural features and then add other characteristics to that for comparison purposes.  This project was developed to examine such a concept.  

There were several objectives I wanted to acheive during this course.  First, I wanted to develop a predictive model that could potentially be applied in a similar geographical areas.  I also wanted to explore various regression methods and functions.  Another goal was to identify which features were most important to the model.  Therefore, I formulated a class project to apply a linear regression machine learning techniques to predict the value of a home based on its structural features.

The project consisted of using data from the Kaggle website titled Zillow "properties_2016".  The project is limited to structural features so that this model may be applicable in other similar geographical areas (with additional data from datasets in different geographical areas).  

The prepatory phase for developing the prediction model included importing data, exploration of the data, selecting the features, and data cleansing.  Regression models were created using various regression functions in R and compared.  This paper decribes each step in the process along with the code and output.  You may view a twelve and a half minute video summarizint this project on YouTube at https://www.youtube.com/watch?v=-yyfe9LdPDc&feature=youtu.be.

```{r include=FALSE}
#load anticipated packages and libraries per Spachtholz (2017) found at https://www.kaggle.com/philippsp/exploratory-analysis-zillow

library(data.table)
library(dtplyr)
library(ggplot2)
library(stringr)
library(DT)
library(tidyr)
library(corrplot)
library(leaflet)
library(lubridate)
library(magrittr)
library(plyr)
library(dplyr)
```

## Dataframe
The real estate data set is the 2016 Zillow properties and was taken from the Kaggle web site.  This data set consisted of 3 million observations on 58 features from homes in the Los Angeles area.  The Zillow data set contained features on various information about a home, such as living area, types of rooms, the presence of a fire place, number of garages, the presence of a pool etc.  It also contained geographical information such as longitude and latitude, land codes, zoning, and region of the city. The dataset properties_2016.csv.zip was obtained from https://www.kaggle.com/c/zillow-prize-1/data.
```{r include=FALSE}
properties_2016 <- read.csv("C:/Users/maria/Desktop/data/properties_2016.csv", stringsAsFactors=TRUE)
```

# Exploratory Data Analysis and Data Cleansing
The exploratory data analysis included examining the data frame for feature definitions, missing values, duplicate features and extreme values.  This data set containing all of these issues.  The first step was to view the structure of the dataframe and is shown below.
```{r echo=FALSE}
str(properties_2016)
```

The goal of this project is to predict the value of a home from its structural features.  The response variable is the column  "structuretaxvaluedollarcnt"" in the original dataframe, which is continuous type data.  The machine learning model will be a multiple linear regression model as recommended by Lantz (2015 p. 21).  The next step was to create a new variable for dataframe to preserve original dataset.
```{r}
properties <- properties_2016
```


```{r include=FALSE}

head(properties)
```

Next, I changed column names to more sensical names as done by Spachtholz (2017) found at https://www.kaggle.com/philippsp/exploratory-analysis-zillowin Kaggle except used building_value label for predictor.

```{r include=FALSE}
properties <- properties %>% rename(
  id_parcel = parcelid,
  build_year = yearbuilt,
  area_basement = basementsqft,
  area_patio = yardbuildingsqft17,
  area_shed = yardbuildingsqft26, 
  area_pool = poolsizesum,  
  area_lot = lotsizesquarefeet, 
  area_garage = garagetotalsqft,
  area_firstfloor_finished = finishedfloor1squarefeet,
  area_total_calc = calculatedfinishedsquarefeet,
  area_base = finishedsquarefeet6,
  area_live_finished = finishedsquarefeet12,
  area_liveperi_finished = finishedsquarefeet13,
  area_total_finished = finishedsquarefeet15,  
  area_unknown = finishedsquarefeet50,
  num_unit = unitcnt, 
  num_story = numberofstories,  
  num_room = roomcnt,
  num_bathroom = bathroomcnt,
  num_bedroom = bedroomcnt,
  num_bathroom_calc = calculatedbathnbr,
  num_bath = fullbathcnt,  
  num_75_bath = threequarterbathnbr, 
  num_fireplace = fireplacecnt,
  num_pool = poolcnt,  
  num_garage = garagecarcnt,  
  region_county = regionidcounty,
  region_city = regionidcity,
  region_zip = regionidzip,
  region_neighbor = regionidneighborhood,  
  tax_total = taxvaluedollarcnt,
  building_value = structuretaxvaluedollarcnt,
  tax_land = landtaxvaluedollarcnt,
  tax_property = taxamount,
  tax_year = assessmentyear,
  tax_delinquency = taxdelinquencyflag,
  tax_delinquency_year = taxdelinquencyyear,
  zoning_property = propertyzoningdesc,
  zoning_landuse = propertylandusetypeid,
  zoning_landuse_county = propertycountylandusecode,
  flag_fireplace = fireplaceflag, 
  flag_tub = hashottuborspa,
  quality = buildingqualitytypeid,
  framing = buildingclasstypeid,
  material = typeconstructiontypeid,
  deck = decktypeid,
  story = storytypeid,
  heating = heatingorsystemtypeid,
  aircon = airconditioningtypeid,
  architectural_style= architecturalstyletypeid
)

```


```{r include=FALSE}
#Verification of the new column headings.
head(properties)
```


```{r include=FALSE}
#Examine the structure of the properties dataset and confirm feature types.
str(properties)
```


```{r include=FALSE}
#Examine the summary of the properties dataset
summary(properties)
```


One component of the exploratory data analysis was to check for duplicate features and values.  There were two columns that appeared similar and they are:  area_total_calc and area_total_finished. This was verified by looking at the head and tail of those columns.   The feature area_total_calc has fewer missing values so I will remove area_total_finished later.
```{r include=FALSE}
summary(properties$area_total_calc)
```

```{r include=FALSE}
head(properties$area_total_calc, 50)
```

```{r include=FALSE}
head(properties$area_total_finished, 50)
```

```{r include=FALSE}
tail(properties$area_total_finished, 100)
```

```{r include=FALSE}
tail(properties$area_total_finished, 100)
```

```{r include=FALSE}
summary(properties$area_total_finished)
```

I also checked the response variable building_value for missing values, to see how many there were.
```{r}
sum(is.na(properties$building_value)) 
```
There were 54,982 observations with missing vaues.  I decided to delete the rows with missing values for response feature as they are only 2% of the rows in the data set.  This code is from Clemens (2018).  Then I created a new dataset without missing values.

```{r include=FALSE}
properties_no_building_na <- filter(properties[!is.na(properties$building_value), ]) 
rownames(properties_no_building_na) <- 1:nrow(properties_no_building_na) # code from James 2011 at https://stackoverflow.com/questions/7567790/change-the-index-number-of-a-dataframe; This source stated that R would insert rows of NAs anywhere there was a missing index in the sequence.  So to prevent mysterious NAs when removing outliers this command re-indexes the dataframe.
```
```{r include=FALSE}
tail(properties_no_building_na)
```

I verifed that the rows with NA in this column were removed.
```{r}
sum(is.na(properties_no_building_na$building_value))
```

```{r include=FALSE}
require(ggplot2)
library(purrr)
```

The next step was to check percent of missing values for all features and the code for this step was taken from Walters (2017).
```{r include=FALSE}
miss_pct <- map_dbl(properties_no_building_na, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })
miss_pct <- miss_pct[miss_pct > 0]
data.frame(miss=miss_pct, var=names(miss_pct), row.names=NULL) %>%
  ggplot(aes(x=reorder(var, -miss), y=miss)) + 
  geom_bar(stat='identity', fill='red') +
  labs(x='', y='% missing', title='Percent missing data by feature') +
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

```{r include=FALSE}
# to view percentages for missing values or NAs
miss_pct
```

The goal for this project is to predict the value of the building based on its structural features.  I had to remove obvious features that are not related to the structure of the home.  This was determined based on the definitions of each feature provided in an excel spreadsheet.  The spread sheet with the definitions for each variable was provided on the Zillow website and was taken from https://www.kaggle.com/philippsp/exploratory-analysis-zillow/data, 

To remove the non-structural features (including building_year) I used the code  from https://stackoverflow.com/questions/5234117/how-to-drop-columns-by-name-in-a-data-frame.  I also removed the feature area_live_finished which appears to be a duplicate column of area_total_calc.

```{r}
struct_features_only_data <- within(properties_no_building_na, rm('fips', 'latitude', 'longitude', 'area_lot', 'id_parcel', 'zoning_landuse_county', 'zoning_landuse', 'zoning_property', 'rawcensustractandblock', 'censustractandblock', 'region_county', 'region_city', 'region_zip', 'region_neighbor', 'area_shed', 'tax_total', 'tax_land', 'tax_property', 'tax_year', 'tax_delinquency', 'tax_delinquency_year', 'num_pool', 'area_pool', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'build_year', 'area_live_finished'))
```

The new dataset struture is listed below.
```{r echo=FALSE}
#view new data set
str(struct_features_only_data)
```

I checked to see if there any complete cases in the new dataset and if so, the number of them to bypass extensive data cleansing.

```{r}
sum(complete.cases(struct_features_only_data))
```
No complete cases,  I proceeded with exploratory data anaysis (EDA) and data cleansing.

The next step was to examine the amount of missing data in the new working data set with structural features only.
```{r echo=FALSE}
#Display the features and their percentages of missing values.
#code below was taken from Walters (2017) at https://www.kaggle.com/captcalculator/a-very-extensive-zillow-exploratory-analysis
miss_pct <- map_dbl(struct_features_only_data, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })
miss_pct <- miss_pct[miss_pct > 0]
data.frame(miss=miss_pct, var=names(miss_pct), row.names=NULL) %>%
  ggplot(aes(x=reorder(var, -miss), y=miss)) + 
  geom_bar(stat='identity', fill='red') +
  labs(x='', y='% missing', title='Percent missing data by feature') +
  theme(axis.text.x=element_text(angle=90, hjust=1))

```

```{r}
miss_pct
```

The next step was to select the features I wanted to include in the model.  I wanted this model to have the potential to be used for any similar geographical areas, so I focused on the structural features of the building, and excluded geographical and neighborhood features. Also excluded were any structural features which contained 26% or more NAs.  This residual sample set contained 6 features.  The response variable is the building value.  The predictors are the number of bathrooms, the total finished living area, the number of bedrooms, the presence of a fireplace and the presence of a hot tub or spa.  The structure and summary of the dataset are shown below.

```{r include=FALSE}
#keep features 25% or less NA's
data25percent <- struct_features_only_data[ , c('num_bathroom', 'area_total_calc', 'num_bedroom', 'flag_fireplace', 'flag_tub', 'building_value')]
```

```{r echo=FALSE}
str(data25percent)
```

```{r include=FALSE}
summary(data25percent)
```

In order to run the regression model on this dataframe, all the features must be numeric.  Therefore, I had to change flag_fireplace and flag_tub to binary features for the correlation matrix and the regression model.  The original data set listed the presence of a fire place, hot tub or spa as "true".  I change these character features into a numeric feature where the number one indicated that a fireplace, hot tube or spa were present in the home and zero to indicate otherwise.  Since the homes in this data set are located in Los Angeles, its not surprising that there were few homes with these features.


```{r include=FALSE}
data25percent$flag_fireplace <- sub("^$", "false", data25percent$flag_fireplace) # code from Hohenstein https://stackoverflow.com/questions/21243588/replace-blank-cells-with-character
table(data25percent$flag_fireplace)

```

```{r echo=FALSE}
data25percent$flag_fireplace <- ifelse(data25percent$flag_fireplace == "true", 1, 0)
table(data25percent$flag_fireplace)
```

```{r include=FALSE}
#verifying transition to binary numbers
tail(data25percent)
```


```{r include=FALSE}
#Changing the feature flag_tub to binary feature.
data25percent$flag_tub <- sub("^$", "false", data25percent$flag_tub)
table(data25percent$flag_tub)
```

```{r echo=FALSE}
data25percent$flag_tub <- ifelse(data25percent$flag_tub == "true", 1, 0)
table(data25percent$flag_tub)
```

Verifying the change to numeric.
```{r echo=FALSE}
head(data25percent)
```

I then checked the class type of each variable in preparation for regression modeling.
```{r echo=FALSE}
#checking the class type of each variable in preparation for regression modeling.
sapply(data25percent, class)
```

## Correlations 

I ran a correlation matrix on the first dataset to see which features had a mild correlation with the response variable "building value".  This correlation shows that three predictors had at least a weak to moderate correlation. They are the number of bedrooms, the number of bathrooms and the area_total_calc.  According to Lantz (2015, p. 180) in the Chapter on Regression Methods, a weak correlation are values between 0.1 and 0.3, where as a moderate correlation is between 0.3 and 0.5.  Strong correlations are values above 0.5. Despite these recommendations, I used all 5 predictors for the linear model to see how this played out.  The code was taken from RDocumentation at https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/cor.

```{r echo=FALSE}
cor(data25percent, use = "pairwise.complete.obs")
```

## Outliers

The next step was to remove the outliers within each feature.  During this phase I encountered a bug in R studio.  As I was removing the outliers for each feature, new NAs were added to other features in the form of new rows.  I could see this in the summary of the data set.  I tried the suggested remedies found on a Stackoverflow post at Subsetting R dataframe results in mysterious NA rows (n.d), such as using the filter(), using the subset() and recounting the rows but was unsuccessful. 

I decided to use an outlier function I found on the internet and planned to removed all NAs since I had a large number of observations.  I used a function from Dhana (2016) to remove outliers.  This function provideds the mean, bloxplots and histogram before and after the outliers are remove.  It will seek a response to the question "Do you want to replace outliers with NAs".  Once this is answered in the console it completes the function.
```{r include=FALSE}
outlierKD <- function(dt, var) {
     var_name <- eval(substitute(var),eval(dt))
     na1 <- sum(is.na(var_name))
     m1 <- mean(var_name, na.rm = T)
     par(mfrow=c(2, 2), oma=c(0,0,3,0))
     boxplot(var_name, main="With outliers")
     hist(var_name, main="With outliers", xlab=NA, ylab=NA)
     outlier <- boxplot.stats(var_name)$out
     mo <- mean(outlier)
     var_name <- ifelse(var_name %in% outlier, NA, var_name)
     boxplot(var_name, main="Without outliers")
     hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
     title("Outlier Check", outer=TRUE)
     na2 <- sum(is.na(var_name))
     cat("Outliers identified:", na2 - na1, "n")
     cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "n")
     cat("Mean of the outliers:", round(mo, 2), "n")
     m2 <- mean(var_name, na.rm = T)
     cat("Mean without removing outliers:", round(m1, 2), "n")
     cat("Mean if we remove outliers:", round(m2, 2), "n")
     response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
     if(response == "y" | response == "yes"){
          dt[as.character(substitute(var))] <- invisible(var_name)
          assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
          cat("Outliers successfully removed", "n")
          return(invisible(dt))
     } else{
          cat("Nothing changed", "n")
          return(invisible(var_name))
     }
}
```

The outlier function applied to the building value and displayed in the graphs below
```{r echo=FALSE, fig.width=4, fig.height=4}
outlierKD(data25percent, building_value)
```
The above plots show the boxplots and histogram before and after the outliers were removed.  The histogram after the building_value were removed is skewed to the right.

The outliers are remvoed from the num_bathroom features and shown in the graphs below
```{r echo=FALSE, fig.width=4, fig.height=4}
outlierKD(data25percent, num_bathroom)
```

Check the summary to confirm outliers are remoed.
```{r echo=FALSE}
summary(data25percent)
```

Removing outliers from area_total_calc with function and are shown in the graphs below.
```{r echo=FALSE, fig.width=4, fig.height=4}
outlierKD(data25percent, area_total_calc)
```

Remove outliers from num_bedroom with function and shown in the graphs below.
```{r echo=FALSE, fig.width=4, fig.height=4}
outlierKD(data25percent, num_bedroom)
```

Check the summary after all outliers were removed.
```{r echo=FALSE}
summary(data25percent)
```

```{r include=FALSE}
str(data25percent)
```


The next step was to identify the number of complete cases after outliers have been removed from all features in data25percent dataframe.

```{r}
sum(complete.cases(data25percent))
```

Then a new dataframe with complete cases was created.
```{r include=FALSE}
complete_data25percent <- na.omit(data25percent)
```

Confirm the structure of the latest dataset.
```{r echo=FALSE}
str(complete_data25percent)
```


Recheck correlation with clean dataframe complete_data25percent

```{r echo=FALSE}
cor(complete_data25percent)
```
The correlation between the num_bedroom and the building value improved, otherwise it is the same.

# Regression Models And Analysis

The next step in the process was to run a multiple linear regression model.  The lm() function from the MASS library was utilized for this step.  In this model the building value is the response variable and the other features are the predictors. 

```{r include=FALSE}
library(MASS)
```

```{r include=FALSE}
summary(complete_data25percent)
```


The first model I ran was the linear model code with lm() on the entire dataset to see if it would work.
```{r echo=FALSE}
model_1 <- lm(building_value ~ ., data = complete_data25percent)
```

Then, I viewed the summary and was dissapointed with the adjusted R_squared and residual errors.
```{r echo=FALSE}
summary(model_1)
```

I checked the AIC score for comparison with later models as described by Prabhakaran, S. (2017). 

```{r}
AIC(model_1)
```

The next step was to create diagnostic plot of the regression model. Interpreting the plots is difficult with such a large number of observations but seems to show that the model had a fairly good fit with the data and there were no outliers outside of Cook's distance.
```{r}
# not enough memory on computer to run during knit process
# par(mfrow=c(2,2))
# plot(model_1)

```

## Create Training and Test Set

I used the code from Prabhakaran (2017) to create the training and test datasets. The training and test sets consisted of an 80 : 20 split.  
```{r include=FALSE}
set.seed(100)
trainingRowIndex <- sample(1:nrow(complete_data25percent), 0.8*nrow(complete_data25percent))
```

```{r include=FALSE}
trainingData <- complete_data25percent[trainingRowIndex, ]
```


```{r include=FALSE}
head(trainingData)
```

I then checked the structure of the training data to confirm the correct number of rows.
```{r echo=FALSE}
str(trainingData)
```


```{r include=FALSE}
testData  <- complete_data25percent[-trainingRowIndex, ]
```

```{r include=FALSE}
head(testData)
```

The next step was to build the model on the training data set and view the summary.
```{r include=FALSE}
model_2 <- lm(building_value ~ ., data=trainingData)
```

```{r echo=FALSE}
summary(model_2)
```

The results from the this model are shown above.  The Adjusted R-square is 0.4359.  This model explains 43.6% of the variation in the dependent variable (Lantz, 2015).  I had hoped this value to be higher.  The majority of the predictions are between the 1st and 3rd quartile, and were between 36,507 over the true value and 32,351 under the true value  (Lantz, 2015).  The overall model  appears to be statistically significant (Prabhakaran 2017) as evidenced by the p values for each predictor and the F-statistic for the model which are less than the significance level of 0.05.

I then ran the AIC and BIC scores to compare with the first model.

```{r}
AIC(model_2)
```
The AIC score improved.

```{r}
BIC(model_2)
```

Next, I ran the model on test data to get the predictions.
```{r}
building_valuePred <- predict(model_2, testData)
```


```{r include=FALSE}
summary(building_valuePred)
```

The next step was to look at the accuracy of the predicted values.  According to Prabhakaran (2017), there are several measures which can be used to look at prediction accuracy.  Two of them are shown here.  The first measure is the correlation between the actual values and the predicted values.  According to this source, a good correlation  "implies that the actual values and the predicted values show similar directional movement" (Prabhakaran, 2017) , which is seen here. 
The first step was to make a dataframe with the actual test values of building_value and the predicted building_values.
```{r}
actuals_preds <- data.frame(cbind(actuals=testData$building_value, predicteds=building_valuePred)) 
```

```{r}
correlation_accuracy <- cor(actuals_preds)
head(actuals_preds)
```

The correlation accuracy as shown below.
```{r}
correlation_accuracy
```
The above correlation between the actual values and the predicted is considered good by Lantz (2015).

The second value is the  Min Max Accuracy.  The closer the higher the score the better according to Prabhakaran (2017).  The Min Max Accuracy score was 0.71.

```{r include=FALSE}
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max)) 
```

```{r include=FALSE}
min_max_accuracy
```


Next, I checked for multi-colinearity of the regression model using the vif function:
```{r include=FALSE}
library(car)

```

```{r echo=FALSE}
vif(model_2)
```
There was no multi-colinearity.

### Various Linear Regression Models

### LM with Cross Validation
The next phase consisted of exploring different linear model functions to see if it changed the overall performance.  I started with the lm with cross Validation as suggested by Prabhakaran (2017).  I chose to use the Caret package for the lm with cross validation.

```{r include=FALSE}
library(caret)
```

```{r include=FALSE}
set.seed(100)
```


The following code for the linear model with cross validation was taken from Datacamp (2016) at https://www.youtube.com/watch?v=OwPQHmiJURI.
```{r include=FALSE}
model3_caret <- train(building_value ~ ., trainingData, method = "lm", trControl = trainControl(method = "cv",  number = 10, verboseIter = FALSE))
```

```{r include=FALSE}
print(model3_caret)
```

```{r echo=FALSE}
summary(model3_caret)
```
The results of the above model were consistant with the previous models.  Next, I looked at the variable importance for the above model since it was within the caret package.

### Variable Importance

One of my objectives for this project was to identify which features were most important in the prediction model.  This was achieve with the variable importance function in the caret package as described by Kaushik (2016).  The results showed that the Total finished Living Area was the most important feature followed by the number of bathrooms and then the number of bedrooms.  Hot tubs and fireplaces were, as expected for the warm climate of Los Angeles, very low value predictors.

```{r echo=FALSE}
varImp(object = model3_caret)
```

```{r echo=FALSE, fig.width=4, fig.height=4}
plot(varImp(object=model3_caret),main="Variable Importance")
```


### GLMNET
Next, I decided to try the glmnet() in Caret to see if this model performed better.  This is a generalized linear model with regularization techniques.
```{r include=FALSE}
set.seed(100)
```

```{r include=FALSE}
library(caret)
glmnet1 <- train(building_value ~ ., trainingData, method = "glmnet", trControl = trainControl(method = "cv",  number = 10, verboseIter = FALSE))
```

```{r echo=FALSE}
glmnet1
```
The above summary showed similar RMSE and R-aquared values.


# Improve Model

### Removing Features
I decided to experiment with the dataset to see if I could improve the performance by altering the number of features in the dataset.  I first tried removing features one by one to see if it improves the model.  I decided to use the Caret package for this.  The following code was taken from Datacamp (2016) at https://www.youtube.com/watch?v=OwPQHmiJURI.

```{r}
model4_caret <- train(building_value ~ area_total_calc + num_bedroom + num_bathroom, trainingData, method = "lm", trControl = trainControl(method = "cv",  number = 10, verboseIter = FALSE))
```

```{r include=FALSE}
model4_caret
```


```{r echo=FALSE}
summary(model4_caret)
```

The above model's performance was worse.

I also used the glmnet package in caret on with the 3 independent variables
```{r}
glmnet2 <- train(building_value ~ area_total_calc + num_bedroom + num_bathroom, trainingData, method = "glmnet", trControl = trainControl(method = "cv",  number = 10, verboseIter = FALSE))
```

```{r echo=FALSE}
print(glmnet2)
```

Next, I ran model with 2 independent variables, just to experiment.
```{r}
model5_caret <- train(building_value ~ area_total_calc + num_bedroom, trainingData, method = "lm", trControl = trainControl(method = "cv",  number = 10, verboseIter = FALSE))
```

```{r echo=FALSE}
print(model5_caret)
```
Less features results in higher RMSE and Rsquared, which is not improving the model.  It was time to try adding more features instead.

### Adding Features
I redefined my sample set to include a few more features.   I went back to see which features contained NAs between 25% and 50% and found three more I could add into a new sample set.  Those three were the type of home heating, the quality or condition of the building and the number of units built into the structure such as a duplex or triplex. I added heating, num-unit, quality into a new dataframe to try to improve the model.  Then, I repeated the process for cleaning the second data set.

```{r}
dataset_2 <- struct_features_only_data[ , c('num_bathroom', 'area_total_calc', 'num_bedroom', 'flag_fireplace', 'flag_tub', 'building_value', 'heating', 'num_unit', 'quality')]
```

```{r include=FALSE}
str(dataset_2)
```
```{r}
table(dataset_2$flag_fireplace)
```

```{r include=FALSE}
#Need to change flag_fireplace and flag_tub to binary features.
dataset_2$flag_fireplace <- sub("^$", "false", dataset_2$flag_fireplace) # code from Hohenstein https://stackoverflow.com/questions/21243588/replace-blank-cells-with-character
table(dataset_2$flag_fireplace)

```

```{r include=FALSE}
dataset_2$flag_fireplace <- ifelse(dataset_2$flag_fireplace == "true", 1, 0)
table(dataset_2$flag_fireplace)
```

```{r include=FALSE}
dataset_2$flag_tub <- sub("^$", "false", dataset_2$flag_tub)
table(dataset_2$flag_tub)
```

```{r include=FALSE}
dataset_2$flag_tub <- ifelse(dataset_2$flag_tub == "true", 1, 0)
table(dataset_2$flag_tub)
```


```{r include=FALSE}
#checking the class type of each variable
sapply(dataset_2, class)
```


Checking correlation of new data set
```{r echo=FALSE}
cor(dataset_2, use = "pairwise.complete.obs")
```

```{r include=FALSE}
summary(dataset_2)
```


```{r include=FALSE}
#Removing outliers from num_bathroom
outlierKD(dataset_2, num_bathroom)
```


```{r include=FALSE}
#removing outliers from area_total_calc
outlierKD(dataset_2, area_total_calc)
```


```{r include=FALSE}
#removing outliers from num_bedroom
outlierKD(dataset_2, num_bedroom)
```


```{r include=FALSE}
#Removing outliers from building_value
outlierKD(dataset_2, building_value)
```


```{r include=FALSE}
#Removing outliers from heating
outlierKD(dataset_2, heating)
```

```{r include=FALSE}
summary(dataset_2$num_unit)
```

```{r include=FALSE}
#Removing outliers from num_unit.
outlierKD(dataset_2, num_unit)
```


```{r include=FALSE}
#Removing outliers quality feature.
summary(dataset_2$quality)
```


```{r include=FALSE}
outlierKD(dataset_2, quality)
```

A summary of the data set at this point is shown below.
```{r echo=FALSE}
summary(dataset_2)
```

Next, I identified number of complete cases.
```{r}
sum(complete.cases(dataset_2))
```

```{r include=FALSE}
str(dataset_2)
```

Then, I removed incomplete cases from the second data set.
```{r}
dataset_2_complete <- na.omit(dataset_2)
```

```{r include=FALSE}
str(dataset_2_complete)
```

I checked the summary of the second data set.
```{r echo=FALSE}
#verifying that NAs are removed
summary(dataset_2_complete)
```

A training set and a test set were created with an 80:20 split.
```{r include=FALSE}
set.seed(100)
trainingRowIndex2 <- sample(1:nrow(dataset_2_complete), 0.8*nrow(dataset_2_complete))
```


```{r include=FALSE}
trainingData2 <- dataset_2_complete[trainingRowIndex2, ]
```

```{r include=FALSE}
testData2  <- dataset_2_complete[-trainingRowIndex2, ]
```


### Linear Regression on trainingData2

Once the second data set was cleaned I was ready to run the regression models again using the same functions as I did with initial data set.
```{r include=FALSE}
fit_A <- lm(building_value ~ ., data=trainingData2)
```

```{r echo=FALSE}
summary(fit_A)
```
I noted some improvement in the RSE and Adjusted R-square.  The features, flag_fireplace and num_unit are not significant predictors, so I removed them from the data set and updated the train and test set.

```{r include=FALSE}
#Remove num_unit and flag_fireplace 
trainingData3 <- within(trainingData2, rm('num_unit', 'flag_fireplace')) 
```


```{r include=FALSE}
testData3 <- within(testData2, rm('num_unit', 'flag_fireplace'))
```

The structure of new training set.
```{r echo=FALSE}
str(trainingData3)
```

Next, I ran the lm() on the revised dataset.
```{r include=FALSE}
fit_C <- lm(building_value ~ ., data=trainingData3)
```

```{r echo=FALSE}
summary(fit_C)
```
The results of the lm model haven't changed with the removal of the flag_fireplace and num_unit.

### Calcualte Metrics

Calcuating AIC and BIC on the model with 6 predictors.
```{r}
AIC(fit_C)
```

```{r}
BIC(fit_C)
```
AIC and BIC values improved compared to initial dataset with 5 predictors.

The next step was to run the model on testData3 set to get predictions.

```{r}
building_valuePred3 <- predict(fit_C, testData3)
```

```{r}
summary(building_valuePred3)
```

The prediction accuracy metrics were then calcuated for a comparison.

```{r include=FALSE}
#Make a dataframe with the actual test values of building_value and the predicted building_values
actuals_preds3 <- data.frame(cbind(actuals=testData3$building_value, predicteds=building_valuePred3)) 
```


```{r include=FALSE}
correlation_accuracy3 <- cor(actuals_preds3)
head(actuals_preds)
```
The correlation accuracy improved to 0.70 and the Min Max Accuracy improved to 0.72,
```{r include=FALSE}
correlation_accuracy3
```
Check the min_max_accuracy.

```{r include=FALSE}
min_max_accuracy2 <- mean(apply(actuals_preds3, 1, min) / apply(actuals_preds3, 1, max)) 
```

```{r include=FALSE}
min_max_accuracy2
```

### LM with Cross Validation on 6 Predictors
The following step is to check model performance using lm with cross validation on second dataset.
```{r}
Fit_B_lm <- train(building_value ~ ., trainingData3, method = "lm", trControl = trainControl(method = "cv",  number = 10, verboseIter = FALSE))
```

```{r echo=FALSE}
print(Fit_B_lm)
```
The above results are Similar results to lm().

I also looked for muti-colinearity
```{r echo=FALSE}
vif(fit_C)
```
There is no multi-colinearity.


I also ran the glmnet function on second dataset and the results were similar.
```{r include=FALSE}
Fit_C_glmnet <- train(building_value ~ ., trainingData3, method = "glmnet", trControl = trainControl(method = "cv",  number = 10, verboseIter = FALSE))
```

```{r include=FALSE}
print(Fit_C_glmnet)
```
Results are consistent.

I rechecked the variable importance on the model from the second data set and created a plot.
```{r include=FALSE}
varImp(object = Fit_B_lm)
```

```{r echo=FALSE}
plot(varImp(object=Fit_B_lm),main="Variable Importance New Data Frame")
```
The above plot is similar to the first variable importance plot where area_total_calc has the highest importance, followed by the quality feature and then the num_bathroom.




![Comparison of Regression Models.](C:\Users\maria\Desktop\Practicum 1\comparisonchart.PNG)

## Discussion
In conclusion, a multiple linear regression model was created to predict the value of a home.  The results of the model improved when more features were added to the dataset.  The type of linear function utlized did not alter the performance of the model, but note that the tuning parameters were not adjusted.  The model with the best performance was the one with the second data set which had 6 predictors.  However, this model requires more work before it can be published for proprietary use as the performance is not satifactory.

Several options are available which may help improve this prediction model furhter.  First, You could continuing to add more features, but by including more structural features there will be more NAs to deal with.  Also, consideration should be give to imputing values or removing more rows with NAs since there is a large number of observations.  Next, manipulating the tuning parameters within regression model fucntion maybe improve results. Another option is to  find a better data set with fewer missing values.  Lastly, to make this model more useful throughout similar geographical areas, additional real estate data from other geographical areas could be obtained and aggregated with the Los Angeles Data.

## References

Clemens (2018) Retrieved from https://stackoverflow.com/questions/48658832/how-to-remove-row-if-it-has-a-na-value-one-certain-column.

Datacamp (2016) Retrieved from https://www.youtube.com/watch?v=OwPQHmiJURI

Dhana, K. (2016).  Identify, describe, plot, and remove the outliers from the dataset.  Retrieved from https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/y

James (2011) Retrieved from  https://stackoverflow.com/questions/7567790/change-the-index-number-of-a-dataframe;

Kaushik, S. (2016). Practical guide to implement machine learning with CARET package in R (with practice problem).  Analytics Vidhya.  Retrieved from https://www.analyticsvidhya.com/blog/2016/12/practical-guide-to-implement-machine-learning-with-caret-package-in-r-with-practice-problem/

Lantz, B. (2015). Chapter 6: Forecasting Numeric Data - Regression Methods.  In B. Lantz, Machine Learning with R Second Edition - Classification Using Decision Trees and Rules (pp. 171-218).   Packt Publishing.

Prabhakaran, S. (2017).  Linear Regression.  R-statistics.co.  Retrieved from http://r-statistics.co/Linear-Regression.html

RDocumentation (n.d.) Retrieved from https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/cor.

Spachtholz (2017) Retrieved from https://www.kaggle.com/philippsp/exploratory-analysis-zillow

Subsetting R data frame results in mysterious NA rows (n.d) Retrieved from https://stackoverflow.com/questions/14261619/subsetting-r-data-frame-results-in-mysterious-na-rows


Walters, T. (2017). A very extensive Zillow exploratory analysis.  Retrieved from https://www.kaggle.com/captcalculator/a-very-extensive-zillow-exploratory-analysis

Zillow Prize:  Zillow's Home Value Prediction(Zestimate) (2018).  Properties_2016.  Retrieved from https://www.kaggle.com/c/zillow-prize-1/data

### Appendix
I ran the lm() with log of response variable as shown in the steps below.  However, I am not as compfortable manipulating or interpreting the log value, so I did not include it in the discussion.

```{r}
m2 <- lm(log(building_value) ~ ., data = trainingData)
```

```{r}
summary(m2)
```
Residuals may have improved the median is closer to zero, Residual Standar error is lower, but also the Adjusted R-squared.  Not sure how to interpret this.

Run log model on test set
```{r}
building_valuePred_log <- predict(m2, testData)
```

Calculate prediction accuracy
Make a dataframe with the actual test values of building_value and the predicted building_values
```{r}
actuals_preds_log <- data.frame(cbind(actuals=testData$building_value, predicteds=building_valuePred_log)) 
```

```{r}
correlation_accuracy_log <- cor(actuals_preds_log)
head(actuals_preds_log)
```

```{r}
correlation_accuracy_log
```
Using the log did not have any better than previous models.



